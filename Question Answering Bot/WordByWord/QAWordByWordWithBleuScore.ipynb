{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport yaml\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, activations, models, preprocessing\nfrom tensorflow.keras import preprocessing, utils\nimport re\nimport matplotlib.pyplot as plt\nfrom nltk.translate.bleu_score import corpus_bleu\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tf.__version__)\n\ndir_path = '/kaggle/input/chatterbotenglisch'\nfiles_list = os.listdir(dir_path + os.sep)\n\nbatch_size = 16  # Batch size for training.\nepochs = 120  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nembedding_dim = 100\nval = 10","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"questions = list()\nanswers = list()\n\nfor filepath in files_list:\n    stream = open(dir_path + os.sep + filepath, 'rb')\n    docs = yaml.safe_load(stream)\n    conversations = docs['conversations']\n    for con in conversations:\n        if len(con) > 2:\n            questions.append(con[0])\n            replies = con[1:]\n            ans = ''\n            for rep in replies:\n                ans += ' ' + rep\n            answers.append(ans)\n        elif len(con) > 1:\n            questions.append(con[0])\n            answers.append(con[1])\n\nanswers_with_tags = list()\nfor i in range(len(answers)):\n    if type(answers[i]) == str:\n        answers_with_tags.append(answers[i])\n    else:\n        questions.pop(i)\n\nanswers = list()\nfor i in range(len(answers_with_tags)):\n    answers.append('<START> ' + answers_with_tags[i] + ' <END>')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def tokenize(sentences):\n    sentences_clear = []\n    for sentence in sentences:\n        sentence = sentence.lower()\n        sentence = re.sub('can\\'t', 'can not', sentence)\n        sentence = re.sub('n\\'t', ' not', sentence)\n        sentence = re.sub('\\'ve', ' have', sentence)\n        sentence = re.sub('\\'ll', ' will', sentence)\n        sentence = re.sub('\\'s', ' is', sentence)\n        sentence = re.sub('\\'m', ' am', sentence)\n        sentence = re.sub('\\'re', ' are', sentence)\n        sentence = re.sub('\\'d', ' would', sentence)\n        sentences_clear.append(sentence)\n    return sentences_clear","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(answers[0])\n\nanswersTok = tokenize(answers)\nquestionsTok = tokenize(questions)\nval_data_count = int(len(answersTok) * val / 100)\ntrain_data_count = len(answersTok) - val_data_count\nprint(answersTok[0])\nprint(\"val_data_count: \", val_data_count)\nprint(\"train_data_count: \", train_data_count)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(questionsTok + answersTok)\nword_index = tokenizer.word_index\nVOCAB_SIZE = len(word_index) + 1\nprint('Found %s unique tokens.' % VOCAB_SIZE)  # Found 56855 unique tokens.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# shuffle\nindices = np.arange(len(questions))\nnp.random.shuffle(indices)\nanswers = list()\nquestions = list()\nfor i in indices:\n    answers.append(answersTok[i])\n    questions.append(questionsTok[i])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO: separate maxlentrain from maxlenval *(optional)and vocabsize also \n# encoder_input_data\ntokenized_questions = tokenizer.texts_to_sequences(questions)\nmaxlen_questions = max([len(x) for x in tokenized_questions])\nprint(maxlen_questions)\n# decoder_input_data\ntokenized_answers = tokenizer.texts_to_sequences(answers)\nmaxlen_answers = max([len(x) for x in tokenized_answers])\nprint(maxlen_answers)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized_questions_train = tokenized_questions[:train_data_count]\ntokenized_answers_train = tokenized_answers[:train_data_count]\nquestions_train = questions[:train_data_count]\nanswers_train = answers[:train_data_count]\ntokenized_questions_val = tokenized_questions[train_data_count+1:]\ntokenized_answers_val = tokenized_answers[train_data_count+1:]\nquestions_val = questions[train_data_count+1:]\nanswers_val = answers[train_data_count+1:]\nprint(len(tokenized_questions_val))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_input_data_train = preprocessing.sequence.pad_sequences(tokenized_questions_train, maxlen=maxlen_questions, padding='post')\ndecoder_input_data_train = preprocessing.sequence.pad_sequences(tokenized_answers_train, maxlen=maxlen_answers, padding='post')\nfor i in range(len(tokenized_answers_train)):\n    tokenized_answers_train[i] = tokenized_answers_train[i][1:]\npadded_answers = preprocessing.sequence.pad_sequences(tokenized_answers_train, maxlen=maxlen_answers, padding='post')\ndecoder_output_data_train = utils.to_categorical(padded_answers, VOCAB_SIZE)\n\nencoder_input_data_val = preprocessing.sequence.pad_sequences(tokenized_questions_val, maxlen=maxlen_questions, padding='post')\ndecoder_input_data_val = preprocessing.sequence.pad_sequences(tokenized_answers_val, maxlen=maxlen_answers, padding='post')\nfor i in range(len(tokenized_answers_val)):\n    tokenized_answers_val[i] = tokenized_answers_val[i][1:]\npadded_answers = preprocessing.sequence.pad_sequences(tokenized_answers_val, maxlen=maxlen_answers, padding='post')\ndecoder_output_data_val = utils.to_categorical(padded_answers, VOCAB_SIZE)\nprint(encoder_input_data_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_inputs = tf.keras.layers.Input(shape=(None,))\nencoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim, mask_zero=True)(encoder_inputs)\nencoder_outputs, state_h, state_c = tf.keras.layers.LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2,\n                                                         return_state=True)(encoder_embedding)\nencoder_states = [state_h, state_c]\n\ndecoder_inputs = tf.keras.layers.Input(shape=(None,))\ndecoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim, mask_zero=True)(decoder_inputs)\ndecoder_lstm = tf.keras.layers.LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2, return_state=True,\n                                    return_sequences=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)\noutput = decoder_dense(decoder_outputs)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['acc'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit([encoder_input_data_train, decoder_input_data_train], decoder_output_data_train, batch_size=batch_size, epochs=epochs,\n                    validation_data=([encoder_input_data_val, decoder_input_data_val],decoder_output_data_val))\nmodel.save('model.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#history = model.fit_generator(generator=my_training_batch_generator\n#                              , steps_per_epoch = int(len(tokenized_questions_train) // batch_size)\n#                              , epochs=epochs\n#                              , validation_data = my_validation_batch_generator\n#                              , validation_steps = int(len(tokenized_questions_val) // batch_size))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next: inference mode (sampling).\n# Here's the drill:\n# 1) encode input and retrieve initial decoder state\n# 2) run one step of decoder with this initial state\n# and a \"start of sequence\" token as target.\n# Output will be the next target token\n# 3) Repeat with the current target token and current states\n\n# Define sampling models\nencoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n\ndecoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,))\ndecoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,))\n\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(\n    decoder_embedding, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = tf.keras.models.Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_word_index = dict(\n    (i, word) for word, i in word_index.items())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def decode_sequence(input_tokens):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_tokens)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1))\n    # Populate the first word of target sequence with the start word.\n    target_seq[0][0] = word_index['start']\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_word = reverse_word_index[sampled_token_index]\n        decoded_sentence += ' ' + sampled_word\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_word == 'end' or\n                len(decoded_sentence.split()) > maxlen_answers):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1))\n        target_seq[0][0] = sampled_token_index\n\n        # Update states\n        states_value = [h, c]\n    if (len('end') > len(decoded_sentence)) and ('end' in decoded_sentence[len(decoded_sentence) - len('end'):]):\n        decoded_sentence = decoded_sentence[:len(decoded_sentence) - len('end')]\n\n    return decoded_sentence","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def str_to_tokens(sentence: str):\n    words = sentence.split()\n    tokens_list = list()\n    for word in words:\n        if word in word_index:\n            tokens_list.append(word_index[word])\n    if len(tokens_list) == 0:\n        return None\n    else:\n        return preprocessing.sequence.pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model\ndef eval_model(raw_questions, raw_answers):\n    predicted, actual = list(), list()\n    for i in range(len(raw_questions)):\n        raw_question = raw_questions[i]\n        tok = str_to_tokens(raw_question)\n        if tok is None:\n            continue\n        translation = decode_sequence(tok)\n        raw_answer = raw_answers[i]\n        raw_answer = raw_answer.replace('<start>','').replace('<end>','')\n        #if(raw_answer[len(raw_answer)])\n        if(\"end\"==translation[len(translation)-3:]):\n            translation = translation[:-3]\n        if i < 10:\n            print('src=[%s], target=[%s], predicted=[%s]' % (raw_question, raw_answer, translation))\n        actual.append([raw_answer.split()])\n        predicted.append(translation.split())\n\n    # Bleu Scores\n    print(\"##############################################\")\n    print(actual[0])\n    print(predicted[0])\n    print(' 1-gram score1: %f' % corpus_bleu(actual, predicted, weights=(1,0,0,0)))\n    print(' 2-gram score1: %f' % corpus_bleu(actual, predicted, weights=(0,1,0,0)))\n    print(' 3-gram score1: %f' % corpus_bleu(actual, predicted, weights=(0,0,1,0)))\n    \n    print(' 4-gram score1: %f' % corpus_bleu(actual, predicted, weights=(0,0,0,1)))\n    print(' 4-gram score2: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Testing on trained examples')\n#print(answers_train)\ndef tok(sentences):\n    sentences_clear = []\n    for sentence in sentences:\n        sentence = sentence.lower()\n        sentence=sentence.replace('<start>','').replace('<end>','')\n        print(sentence)\n        sentence = re.sub('[^\\w\\d\\s]', '', sentence)\n        print(sentence)\n        sentences_clear.append(sentence)\n    return sentences_clear\nanswers_train=tok(answers_train)\nprint(answers_train)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eval_model(questions_train, answers_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Evaluate the model\ndef eval_model(raw_questions, raw_answers):\n    predicted, actual = list(), list()\n    for i in range(len(raw_questions)):\n        raw_question = raw_questions[i]\n        tok = str_to_tokens(raw_question)\n        if tok is None:\n            continue\n        translation = decode_sequence(tok)\n        raw_answer = raw_answers[i]\n        raw_answer = raw_answer.replace('<start>','').replace('<end>','')\n        translation = translation[:-3]\n        if i < 10:\n            print('src=[%s], target=[%s], predicted=[%s]' % (raw_question, raw_answer, translation))\n        actual.append(raw_answer.split())\n        predicted.append(translation.split())\n\n    # Bleu Scores\n    score1 = corpus_bleu(reference, candidate, weights=(1,1,1,1))\n    score2 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n\nprint('Testing on validation examples'\neval_model(questions_val, answers_val) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"exit_pro = False\nwhile not exit_pro:\n    input_seq = input('Enter question or exit : ')\n    input_seq = input_seq.lower().strip()\n    if input_seq == \"exit\":\n        exit_pro = True\n    else:\n        input_tokens = str_to_tokens(input_seq)\n        if decoded_sentence is None:\n            print(\"Sorry, I can't answer this question\")\n        else:\n            decoded_sentence = decode_sequence(input_tokens)\n            print(decoded_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"st=\"you are a cheat end\"\n\nprint(st[:-3]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu\nreference = [[['1','3']],[['1','2']]]\ncandidate = [['1','2'],['1','3']]\n\n\nscore = corpus_bleu(reference, candidate, weights=(1,1,1,1))\nprint(score)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}