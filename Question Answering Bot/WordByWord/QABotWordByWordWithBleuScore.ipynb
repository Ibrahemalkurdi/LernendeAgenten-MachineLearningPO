{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, activations, models, preprocessing\n",
    "from tensorflow.keras import preprocessing, utils\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import corpus_bleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "\n",
    "dir_path = '/kaggle/input/chatterbotenglisch'\n",
    "files_list = os.listdir(dir_path + os.sep)\n",
    "\n",
    "batch_size = 16  # Batch size for training.\n",
    "epochs = 120  # Number of epochs to train for.\n",
    "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
    "embedding_dim = 100\n",
    "val = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = list()\n",
    "answers = list()\n",
    "\n",
    "for filepath in files_list:\n",
    "    stream = open(dir_path + os.sep + filepath, 'rb')\n",
    "    docs = yaml.safe_load(stream)\n",
    "    conversations = docs['conversations']\n",
    "    for con in conversations:\n",
    "        if len(con) > 2:\n",
    "            questions.append(con[0])\n",
    "            replies = con[1:]\n",
    "            ans = ''\n",
    "            for rep in replies:\n",
    "                ans += ' ' + rep\n",
    "            answers.append(ans)\n",
    "        elif len(con) > 1:\n",
    "            questions.append(con[0])\n",
    "            answers.append(con[1])\n",
    "\n",
    "answers_with_tags = list()\n",
    "for i in range(len(answers)):\n",
    "    if type(answers[i]) == str:\n",
    "        answers_with_tags.append(answers[i])\n",
    "    else:\n",
    "        questions.pop(i)\n",
    "\n",
    "answers = list()\n",
    "for i in range(len(answers_with_tags)):\n",
    "    answers.append('<START> ' + answers_with_tags[i] + ' <END>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentences):\n",
    "    sentences_clear = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence = re.sub('can\\'t', 'can not', sentence)\n",
    "        sentence = re.sub('n\\'t', ' not', sentence)\n",
    "        sentence = re.sub('\\'ve', ' have', sentence)\n",
    "        sentence = re.sub('\\'ll', ' will', sentence)\n",
    "        sentence = re.sub('\\'s', ' is', sentence)\n",
    "        sentence = re.sub('\\'m', ' am', sentence)\n",
    "        sentence = re.sub('\\'re', ' are', sentence)\n",
    "        sentence = re.sub('\\'d', ' would', sentence)\n",
    "        sentences_clear.append(sentence)\n",
    "    return sentences_clear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answers[0])\n",
    "\n",
    "answersTok = tokenize(answers)\n",
    "questionsTok = tokenize(questions)\n",
    "val_data_count = int(len(answersTok) * val / 100)\n",
    "train_data_count = len(answersTok) - val_data_count\n",
    "print(answersTok[0])\n",
    "print(\"val_data_count: \", val_data_count)\n",
    "print(\"train_data_count: \", train_data_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(questionsTok + answersTok)\n",
    "word_index = tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index) + 1\n",
    "print('Found %s unique tokens.' % VOCAB_SIZE)  # Found 56855 unique tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "indices = np.arange(len(questions))\n",
    "np.random.shuffle(indices)\n",
    "answers = list()\n",
    "questions = list()\n",
    "for i in indices:\n",
    "    answers.append(answersTok[i])\n",
    "    questions.append(questionsTok[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: separate maxlentrain from maxlenval *(optional)and vocabsize also \n",
    "# encoder_input_data\n",
    "tokenized_questions = tokenizer.texts_to_sequences(questions)\n",
    "maxlen_questions = max([len(x) for x in tokenized_questions])\n",
    "print(maxlen_questions)\n",
    "# decoder_input_data\n",
    "tokenized_answers = tokenizer.texts_to_sequences(answers)\n",
    "maxlen_answers = max([len(x) for x in tokenized_answers])\n",
    "print(maxlen_answers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions_train = tokenized_questions[:train_data_count]\n",
    "tokenized_answers_train = tokenized_answers[:train_data_count]\n",
    "questions_train = questions[:train_data_count]\n",
    "answers_train = answers[:train_data_count]\n",
    "tokenized_questions_val = tokenized_questions[train_data_count+1:]\n",
    "tokenized_answers_val = tokenized_answers[train_data_count+1:]\n",
    "questions_val = questions[train_data_count+1:]\n",
    "answers_val = answers[train_data_count+1:]\n",
    "print(len(tokenized_questions_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data_train = preprocessing.sequence.pad_sequences(tokenized_questions_train, maxlen=maxlen_questions, padding='post')\n",
    "decoder_input_data_train = preprocessing.sequence.pad_sequences(tokenized_answers_train, maxlen=maxlen_answers, padding='post')\n",
    "for i in range(len(tokenized_answers_train)):\n",
    "    tokenized_answers_train[i] = tokenized_answers_train[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers_train, maxlen=maxlen_answers, padding='post')\n",
    "decoder_output_data_train = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
    "\n",
    "encoder_input_data_val = preprocessing.sequence.pad_sequences(tokenized_questions_val, maxlen=maxlen_questions, padding='post')\n",
    "decoder_input_data_val = preprocessing.sequence.pad_sequences(tokenized_answers_val, maxlen=maxlen_answers, padding='post')\n",
    "for i in range(len(tokenized_answers_val)):\n",
    "    tokenized_answers_val[i] = tokenized_answers_val[i][1:]\n",
    "padded_answers = preprocessing.sequence.pad_sequences(tokenized_answers_val, maxlen=maxlen_answers, padding='post')\n",
    "decoder_output_data_val = utils.to_categorical(padded_answers, VOCAB_SIZE)\n",
    "print(encoder_input_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "encoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim, mask_zero=True)(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = tf.keras.layers.LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2,\n",
    "                                                         return_state=True)(encoder_embedding)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "decoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim, mask_zero=True)(decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2, return_state=True,\n",
    "                                    return_sequences=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "decoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)\n",
    "output = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([encoder_input_data_train, decoder_input_data_train], decoder_output_data_train, batch_size=batch_size, epochs=epochs,\n",
    "                    validation_data=([encoder_input_data_val, decoder_input_data_val],decoder_output_data_val))\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#history = model.fit_generator(generator=my_training_batch_generator\n",
    "#                              , steps_per_epoch = int(len(tokenized_questions_train) // batch_size)\n",
    "#                              , epochs=epochs\n",
    "#                              , validation_data = my_validation_batch_generator\n",
    "#                              , validation_steps = int(len(tokenized_questions_val) // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next: inference mode (sampling).\n",
    "# Here's the drill:\n",
    "# 1) encode input and retrieve initial decoder state\n",
    "# 2) run one step of decoder with this initial state\n",
    "# and a \"start of sequence\" token as target.\n",
    "# Output will be the next target token\n",
    "# 3) Repeat with the current target token and current states\n",
    "\n",
    "# Define sampling models\n",
    "encoder_model = tf.keras.models.Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_embedding, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = tf.keras.models.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_word_index = dict(\n",
    "    (i, word) for word, i in word_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_tokens):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_tokens)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0][0] = word_index['start']\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = reverse_word_index[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_word == 'end' or\n",
    "                len(decoded_sentence.split()) > maxlen_answers):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0][0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    if (len('end') > len(decoded_sentence)) and ('end' in decoded_sentence[len(decoded_sentence) - len('end'):]):\n",
    "        decoded_sentence = decoded_sentence[:len(decoded_sentence) - len('end')]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_tokens(sentence: str):\n",
    "    words = sentence.split()\n",
    "    tokens_list = list()\n",
    "    for word in words:\n",
    "        if word in word_index:\n",
    "            tokens_list.append(word_index[word])\n",
    "    if len(tokens_list) == 0:\n",
    "        return None\n",
    "    else:\n",
    "        return preprocessing.sequence.pad_sequences([tokens_list], maxlen=maxlen_questions, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def eval_model(raw_questions, raw_answers):\n",
    "    predicted, actual = list(), list()\n",
    "    for i in range(len(raw_questions)):\n",
    "        raw_question = raw_questions[i]\n",
    "        tok = str_to_tokens(raw_question)\n",
    "        if tok is None:\n",
    "            continue\n",
    "        translation = decode_sequence(tok)\n",
    "        raw_answer = raw_answers[i]\n",
    "        raw_answer = raw_answer.replace('<start>','').replace('<end>','')\n",
    "        #if(raw_answer[len(raw_answer)])\n",
    "        if(\"end\"==translation[len(translation)-3:]):\n",
    "            translation = translation[:-3]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_question, raw_answer, translation))\n",
    "        actual.append([raw_answer.split()])\n",
    "        predicted.append(translation.split())\n",
    "\n",
    "    # Bleu Scores\n",
    "    print(\"##############################################\")\n",
    "    print(actual[0])\n",
    "    print(predicted[0])\n",
    "    print(' 1-gram score1: %f' % corpus_bleu(actual, predicted, weights=(1,0,0,0)))\n",
    "    print(' 2-gram score1: %f' % corpus_bleu(actual, predicted, weights=(0,1,0,0)))\n",
    "    print(' 3-gram score1: %f' % corpus_bleu(actual, predicted, weights=(0,0,1,0)))\n",
    "    \n",
    "    print(' 4-gram score1: %f' % corpus_bleu(actual, predicted, weights=(0,0,0,1)))\n",
    "    print(' 4-gram score2: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Testing on trained examples')\n",
    "#print(answers_train)\n",
    "def tok(sentences):\n",
    "    sentences_clear = []\n",
    "    for sentence in sentences:\n",
    "        sentence = sentence.lower()\n",
    "        sentence=sentence.replace('<start>','').replace('<end>','')\n",
    "        print(sentence)\n",
    "        sentence = re.sub('[^\\w\\d\\s]', '', sentence)\n",
    "        print(sentence)\n",
    "        sentences_clear.append(sentence)\n",
    "    return sentences_clear\n",
    "answers_train=tok(answers_train)\n",
    "print(answers_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(questions_train, answers_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "def eval_model(raw_questions, raw_answers):\n",
    "    predicted, actual = list(), list()\n",
    "    for i in range(len(raw_questions)):\n",
    "        raw_question = raw_questions[i]\n",
    "        tok = str_to_tokens(raw_question)\n",
    "        if tok is None:\n",
    "            continue\n",
    "        translation = decode_sequence(tok)\n",
    "        raw_answer = raw_answers[i]\n",
    "        raw_answer = raw_answer.replace('<start>','').replace('<end>','')\n",
    "        translation = translation[:-3]\n",
    "        if i < 10:\n",
    "            print('src=[%s], target=[%s], predicted=[%s]' % (raw_question, raw_answer, translation))\n",
    "        actual.append(raw_answer.split())\n",
    "        predicted.append(translation.split())\n",
    "\n",
    "    # Bleu Scores\n",
    "    score1 = corpus_bleu(reference, candidate, weights=(1,1,1,1))\n",
    "    score2 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25))\n",
    "\n",
    "print('Testing on validation examples'\n",
    "eval_model(questions_val, answers_val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exit_pro = False\n",
    "while not exit_pro:\n",
    "    input_seq = input('Enter question or exit : ')\n",
    "    input_seq = input_seq.lower().strip()\n",
    "    if input_seq == \"exit\":\n",
    "        exit_pro = True\n",
    "    else:\n",
    "        input_tokens = str_to_tokens(input_seq)\n",
    "        if decoded_sentence is None:\n",
    "            print(\"Sorry, I can't answer this question\")\n",
    "        else:\n",
    "            decoded_sentence = decode_sequence(input_tokens)\n",
    "            print(decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=\"you are a cheat end\"\n",
    "\n",
    "print(st[:-3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "reference = [[['1','3']],[['1','2']]]\n",
    "candidate = [['1','2'],['1','3']]\n",
    "\n",
    "\n",
    "score = corpus_bleu(reference, candidate, weights=(1,1,1,1))\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
