{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO:\n# bigdata\n# call saved model\n# acc & loss history\n# GUI\n# summary -> snaptool\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mkdir ../output","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"from __future__ import print_function\n\nfrom keras.models import Model\nfrom keras.layers import Input, LSTM, Dense\nimport numpy as np\nfrom keras.utils import plot_model\n\nimport os\nimport yaml\nimport re\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 12  # Batch size for training.\nepochs = 100  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dir_path = '../input/mychatbotdataset/chatterbotenglish'\nfiles_list = os.listdir(dir_path + os.sep)\n\ninput_texts = []\ntarget_texts = []\nfor filepath in files_list:\n    stream = open(dir_path + os.sep + filepath, 'rb')\n    docs = yaml.safe_load(stream)\n    conversations = docs['conversations']\n    for con in conversations:\n        if len(con) > 2:\n            input_texts.append(con[0])\n            replies = con[1:]\n            ans = ''\n            for rep in replies:\n                ans += ' ' + rep\n            target_texts.append(ans)\n        elif len(con) > 1:\n            input_texts.append(con[0])\n            target_texts.append(con[1])\n\nanswers_with_tags = list()\nfor i in range(len(target_texts)):\n    if type(target_texts[i]) == str:\n        answers_with_tags.append(target_texts[i])\n    else:\n        input_texts.pop(i)\n\ntarget_texts = list()\nfor i in range(len(answers_with_tags)):\n    target_texts.append('\\t' + answers_with_tags[i] + '\\n')\n\n\ndef tokenize(sentences):\n    # tokens_list = []\n    # vocabulary = []\n    sentences_clear = []\n    for sentence in sentences:\n        sentence = sentence.lower()\n        sentence = re.sub('can\\'t', 'can not', sentence)\n        sentence = re.sub('n\\'t', ' not', sentence)\n        sentence = re.sub('\\'ve', ' have', sentence)\n        sentence = re.sub('\\'ll', ' will', sentence)\n        sentence = re.sub('\\'s', ' is', sentence)\n        sentence = re.sub('\\'m', ' am', sentence)\n        sentence = re.sub('\\'re', ' are', sentence)\n        sentence = re.sub('\\'d', ' would', sentence)\n        sentences_clear.append(sentence)\n        # tokens = sentence.split()\n        # vocabulary += tokens\n        # tokens_list.append(tokens)\n    return sentences_clear\n\n\ntarget_texts = tokenize(target_texts)\ninput_texts = tokenize(input_texts)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_characters = set()\ntarget_characters = set()\nfor i in range(len(input_texts)):\n    for char in input_texts[i]:\n        if char not in input_characters:\n            input_characters.add(char)\n    for char in target_texts[i]:\n        if char not in target_characters:\n            target_characters.add(char)\ninput_characters = sorted(list(input_characters))\ntarget_characters = sorted(list(target_characters))\n\nnum_encoder_tokens = len(input_characters)\nnum_decoder_tokens = len(target_characters)\n\nmax_encoder_seq_length = max([len(txt) for txt in input_texts])\nmax_decoder_seq_length = max([len(txt) for txt in target_texts])\n\nprint('Number of samples:', len(input_texts))\nprint('Number of unique input tokens:', num_encoder_tokens)\nprint('Number of unique output tokens:', num_decoder_tokens)\nprint('Max sequence length for inputs:', max_encoder_seq_length)\nprint('Max sequence length for outputs:', max_decoder_seq_length)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_token_index = dict(\n    [(char, i) for i, char in enumerate(input_characters)])\ntarget_token_index = dict(\n    [(char, i) for i, char in enumerate(target_characters)])\n\nencoder_input_data = np.zeros(\n    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n    dtype='float32')\ndecoder_input_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')\ndecoder_target_data = np.zeros(\n    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n    dtype='float32')\nfor i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n    for t, char in enumerate(input_text):\n        encoder_input_data[i, t, input_token_index[char]] = 1.\n    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n    for t, char in enumerate(target_text):\n        # decoder_target_data is ahead of decoder_input_data by one timestep\n        decoder_input_data[i, t, target_token_index[char]] = 1.\n        if t > 0:\n            # decoder_target_data will be ahead by one timestep\n            # and will not include the start character.\n            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n    decoder_target_data[i, t:, target_token_index[' ']] = 1.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define an input sequence and process it.\nencoder_inputs = Input(shape=(None, num_encoder_tokens))\nencoder = LSTM(latent_dim, return_state=True)\nencoder_outputs, state_h, state_c = encoder(encoder_inputs)\n# We discard `encoder_outputs` and only keep the states.\nencoder_states = [state_h, state_c]\nprint(\"state_h shape: \")  # von mir\nprint(state_h.shape)\nprint(\"state_c shape: \")  # von mir\nprint(state_c.shape)\n\n# Set up the decoder, using `encoder_states` as initial state.\ndecoder_inputs = Input(shape=(None, num_decoder_tokens))\n# We set up our decoder to return full output sequences,\n# and to return internal states as well. We don't use the\n# return states in the training model, but we will use them in inference.\ndecoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n                                     initial_state=encoder_states)\nprint(\"decoder_outputs shape: \")  # von mir\nprint(decoder_outputs.shape)  # von mir\ndecoder_dense = Dense(num_decoder_tokens, activation='softmax')\ndecoder_outputs = decoder_dense(decoder_outputs)\nprint(\"decoder_outputs shape: \")  # von mir\nprint(decoder_outputs.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the model that will turn\n# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\nmodel = Model([encoder_inputs, decoder_inputs], decoder_outputs)\nmodel.summary()  # von mir\n# plot the model\nplot_model(model, to_file='modeCharByCharLikeTrans.png', show_shapes=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Run training\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n          batch_size=batch_size,\n          epochs=epochs,\n          validation_split=0.2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Save model\nmodel.save('CHCharByCharLikeTrans.h5')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_json = model.to_json()\nwith open(\"JCHCharByCharLikeTrans.json\",\"w\") as json_file:\n    json_file.write(model_json)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Next: inference mode (sampling).\n# Here's the drill:\n# 1) encode input and retrieve initial decoder state\n# 2) run one step of decoder with this initial state\n# and a \"start of sequence\" token as target.\n# Output will be the next target token\n# 3) Repeat with the current target token and current states\n\n# Define sampling models\nencoder_model = Model(encoder_inputs, encoder_states)\n\ndecoder_state_input_h = Input(shape=(latent_dim,))\ndecoder_state_input_c = Input(shape=(latent_dim,))\ndecoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\ndecoder_outputs, state_h, state_c = decoder_lstm(\n    decoder_inputs, initial_state=decoder_states_inputs)\ndecoder_states = [state_h, state_c]\ndecoder_outputs = decoder_dense(decoder_outputs)\ndecoder_model = Model(\n    [decoder_inputs] + decoder_states_inputs,\n    [decoder_outputs] + decoder_states)\n\n# Reverse-lookup token index to decode sequences back to\n# something readable.\nreverse_input_char_index = dict(\n    (i, char) for char, i in input_token_index.items())\nreverse_target_char_index = dict(\n    (i, char) for char, i in target_token_index.items())\n\n\ndef decode_sequence(input_seq):\n    # Encode the input as state vectors.\n    states_value = encoder_model.predict(input_seq)\n\n    # Generate empty target sequence of length 1.\n    target_seq = np.zeros((1, 1, num_decoder_tokens))\n    # Populate the first character of target sequence with the start character.\n    target_seq[0, 0, target_token_index['\\t']] = 1.\n\n    # Sampling loop for a batch of sequences\n    # (to simplify, here we assume a batch of size 1).\n    stop_condition = False\n    decoded_sentence = ''\n    while not stop_condition:\n        output_tokens, h, c = decoder_model.predict(\n            [target_seq] + states_value)\n\n        # Sample a token\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = reverse_target_char_index[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        # Exit condition: either hit max length\n        # or find stop character.\n        if (sampled_char == '\\n' or\n                len(decoded_sentence) > max_decoder_seq_length):\n            stop_condition = True\n\n        # Update the target sequence (of length 1).\n        target_seq = np.zeros((1, 1, num_decoder_tokens))\n        target_seq[0, 0, sampled_token_index] = 1.\n\n        # Update states\n        states_value = [h, c]\n\n    return decoded_sentence\n\nfor seq_index in range(100):\n    # Take one sequence (part of the training set)\n    # for trying out decoding.\n    input_seq = encoder_input_data[seq_index: seq_index + 1]\n    decoded_sentence = decode_sequence(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Decoded sentence:', decoded_sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def str_to_tokens(sentence: str):\n    input_seq = np.zeros((1, max_encoder_seq_length, num_encoder_tokens),\n                         dtype='float32')\n    for i in range(min(len(sentence), max_encoder_seq_length)):\n        input_seq[0, i, input_token_index[sentence[i]]] = 1.\n    input_seq[0, i + 1:, input_token_index[' ']] = 1.\n    if len(input_seq) == 0:\n        return None\n    else:\n        return input_seq\n\n\nexit_pro = False\nwhile not exit_pro:\n    input_seq = input('Enter question or exit : ')\n    input_seq = input_seq.lower().strip()\n    if input_seq == \"exit\":\n        exit_pro = True\n    else:\n        input_seq = str_to_tokens(input_seq)\n        if input_seq is None:\n            print(\"Sorry, I can't answer this question\")\n        else:\n            decoded_sentence = decode_sequence(input_seq)\n            print(decoded_sentence)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}