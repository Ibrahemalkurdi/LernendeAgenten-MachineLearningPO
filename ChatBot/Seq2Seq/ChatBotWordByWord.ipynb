{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip uninstall tensorflow -y","execution_count":1,"outputs":[{"output_type":"stream","text":"\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install tensorflow-gpu","execution_count":2,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: tensorflow-gpu in /opt/conda/lib/python3.7/site-packages (2.2.0)\nRequirement already satisfied: tensorboard<2.3.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (2.2.2)\nRequirement already satisfied: astunparse==1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.6.3)\nRequirement already satisfied: absl-py>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (0.9.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (3.2.1)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.0)\nRequirement already satisfied: h5py<2.11.0,>=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (2.10.0)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.14.0)\nRequirement already satisfied: protobuf>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (3.11.4)\nRequirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (0.34.2)\nRequirement already satisfied: google-pasta>=0.1.8 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (0.2.0)\nRequirement already satisfied: numpy<2.0,>=1.16.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.18.1)\nRequirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.4.1)\nRequirement already satisfied: tensorflow-estimator<2.3.0,>=2.2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (2.2.0)\nRequirement already satisfied: grpcio>=1.8.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.29.0)\nRequirement already satisfied: gast==0.3.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (0.3.3)\nRequirement already satisfied: wrapt>=1.11.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.11.2)\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-gpu) (1.1.2)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.6.0.post3)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.23.0)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (46.1.3.post20200325)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.0.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.14.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.2.1)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.2.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (2020.4.5.1)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (1.24.3)\nRequirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (4.0)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.1.1)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.2.7)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (3.0.1)\nRequirement already satisfied: pyasn1>=0.1.3 in /opt/conda/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow-gpu) (0.4.8)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.python.client import device_lib \nprint(device_lib.list_local_devices())","execution_count":3,"outputs":[{"output_type":"stream","text":"[name: \"/device:CPU:0\"\ndevice_type: \"CPU\"\nmemory_limit: 268435456\nlocality {\n}\nincarnation: 13966761671273704624\n, name: \"/device:XLA_CPU:0\"\ndevice_type: \"XLA_CPU\"\nmemory_limit: 17179869184\nlocality {\n}\nincarnation: 9331618332426397510\nphysical_device_desc: \"device: XLA_CPU device\"\n]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nassert tf.__version__.startswith('2')\ntf.random.set_seed(1234)\n\n!pip install tensorflow-datasets==1.2.0\nimport tensorflow_datasets as tfds\n\nimport os\nimport re\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n\n","execution_count":4,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: tensorflow-datasets==1.2.0 in /opt/conda/lib/python3.7/site-packages (1.2.0)\nRequirement already satisfied: protobuf>=3.6.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (3.11.4)\nRequirement already satisfied: promise in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (2.3)\nRequirement already satisfied: tensorflow-metadata in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (0.22.2)\nRequirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (19.3.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (5.7.0)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (1.14.0)\nRequirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (0.18.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (1.18.1)\nRequirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (1.1.0)\nRequirement already satisfied: wrapt in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (1.11.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (2.23.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (4.45.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (0.9.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from tensorflow-datasets==1.2.0) (0.3.1.1)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow-datasets==1.2.0) (46.1.3.post20200325)\nRequirement already satisfied: googleapis-common-protos in /opt/conda/lib/python3.7/site-packages (from tensorflow-metadata->tensorflow-datasets==1.2.0) (1.51.0)\nRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2.9)\nRequirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (2020.4.5.1)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->tensorflow-datasets==1.2.0) (1.24.3)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras import layers, activations, models, preprocessing\nfrom tensorflow.keras import preprocessing, utils","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"path_to_zip = tf.keras.utils.get_file(\n    'cornell_movie_dialogs.zip',\n    origin=\n    'http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip',\n    extract=True)\n\npath_to_dataset = os.path.join(\n    os.path.dirname(path_to_zip), \"cornell movie-dialogs corpus\")\n\npath_to_movie_lines = os.path.join(path_to_dataset, 'movie_lines.txt')\npath_to_movie_conversations = os.path.join(path_to_dataset,\n                                           'movie_conversations.txt')","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Maximum number of samples to preprocess\nMAX_SAMPLES = 100000\n\ndef preprocess_sentence(sentence):\n    sentence = sentence.lower().strip()\n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n    sentence = sentence.strip()\n    # adding a start and an end token to the sentence\n    return sentence\n\n\ndef load_conversations():\n    # dictionary of line id to text\n    id2line = {}\n    with open(path_to_movie_lines, errors='ignore') as file:\n        lines = file.readlines()\n    for line in lines:\n        parts = line.replace('\\n', '').split(' +++$+++ ')\n        id2line[parts[0]] = parts[4]\n    \n    inputs, outputs = [], []\n    with open(path_to_movie_conversations, 'r') as file:\n        lines = file.readlines()\n    print(len(lines))\n    for line in lines:\n        parts = line.replace('\\n', '').split(' +++$+++ ')\n        # get conversation in a list of line ID\n        conversation = [line[1:-1] for line in parts[3][1:-1].split(', ')]\n        for i in range(len(conversation) - 1):\n            inputs.append(preprocess_sentence(id2line[conversation[i]]))\n            outputs.append(preprocess_sentence(id2line[conversation[i + 1]]))\n            if len(inputs) >= MAX_SAMPLES:\n                print(\"True-----\")\n                return inputs, outputs\n    return inputs, outputs\n\n\nquestions, answers = load_conversations()","execution_count":7,"outputs":[{"output_type":"stream","text":"83097\nTrue-----\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Sample question: {}'.format(questions[20]))\nprint('Sample answer: {}'.format(answers[20]))","execution_count":8,"outputs":[{"output_type":"stream","text":"Sample question: i really , really , really wanna go , but i can t . not unless my sister goes .\nSample answer: i m workin on it . but she doesn t seem to be goin for him .\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for an in range(len(answers)):\n    answers[an] = '<START> ' + answers[an] + ' <END>'","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Sample answer: {}'.format(answers[20]))","execution_count":10,"outputs":[{"output_type":"stream","text":"Sample answer: <START> i m workin on it . but she doesn t seem to be goin for him . <END>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = preprocessing.text.Tokenizer()\ntokenizer.fit_on_texts(questions + answers)\nword_index = tokenizer.word_index\nVOCAB_SIZE = len(word_index) + 1\nprint('Found %s unique tokens.' % VOCAB_SIZE)","execution_count":11,"outputs":[{"output_type":"stream","text":"Found 32979 unique tokens.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoder_input_data\ntokenized_questions = tokenizer.texts_to_sequences(questions)\nmaxlen_questions = max([len(x) for x in tokenized_questions])\nprint(maxlen_questions)\n# decoder_input_data\ntokenized_answers = tokenizer.texts_to_sequences(answers)\nmaxlen_answers = max([len(x) for x in tokenized_answers])\nprint(maxlen_answers)","execution_count":12,"outputs":[{"output_type":"stream","text":"299\n505\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64  # Batch size for training.\nepochs = 1  # Number of epochs to train for.\nlatent_dim = 256  # Latent dimensionality of the encoding space.\nembedding_dim = 100","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"encoder_inputs = tf.keras.layers.Input(shape=(None,))\nencoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim, mask_zero=True)(encoder_inputs)\nencoder_outputs, state_h, state_c = tf.keras.layers.LSTM(latent_dim, dropout=0.2,\n                                                         recurrent_dropout=0.2,\n                                                         return_state=True)(encoder_embedding)\nencoder_states = [state_h, state_c]\n\ndecoder_inputs = tf.keras.layers.Input(shape=(None,))\ndecoder_embedding = tf.keras.layers.Embedding(VOCAB_SIZE, embedding_dim, mask_zero=True)(decoder_inputs)\ndecoder_lstm = tf.keras.layers.LSTM(latent_dim, dropout=0.2, recurrent_dropout=0.2, return_state=True,\n                                    return_sequences=True)\ndecoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\ndecoder_dense = tf.keras.layers.Dense(VOCAB_SIZE, activation=tf.keras.activations.softmax)\noutput = decoder_dense(decoder_outputs)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = tf.keras.models.Model([encoder_inputs, decoder_inputs], output)\nmodel.summary()","execution_count":15,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, None)]       0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, None, 100)    3297900     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, None, 100)    3297900     input_2[0][0]                    \n__________________________________________________________________________________________________\nlstm (LSTM)                     [(None, 256), (None, 365568      embedding[0][0]                  \n__________________________________________________________________________________________________\nlstm_1 (LSTM)                   [(None, None, 256),  365568      embedding_1[0][0]                \n                                                                 lstm[0][1]                       \n                                                                 lstm[0][2]                       \n__________________________________________________________________________________________________\ndense (Dense)                   (None, None, 32979)  8475603     lstm_1[0][0]                     \n==================================================================================================\nTotal params: 15,802,539\nTrainable params: 15,802,539\nNon-trainable params: 0\n__________________________________________________________________________________________________\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(optimizer=tf.keras.optimizers.RMSprop(), loss='categorical_crossentropy', metrics=['acc'])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataToTake = 500","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"size_all = len(tokenized_answers)\nprint(size_all)","execution_count":17,"outputs":[{"output_type":"stream","text":"100000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"startIndex =  0\ntoIndex = dataToTake","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"while True:\n    print(startIndex,toIndex)\n    if startIndex >= size_all:\n        break\n    ansBatchToTake = tokenized_answers[startIndex:toIndex]\n    queBatchToTake = tokenized_questions[startIndex:toIndex]\n    startIndex += dataToTake\n    toIndex += dataToTake\n    toIndex = min(toIndex,size_all)\n    # encoder_input_data\n    encoder_input_data = preprocessing.sequence.pad_sequences(queBatchToTake, maxlen=maxlen_questions, padding='post')\n    #print(encoder_input_data.shape)\n    # decoder_input_data\n    decoder_input_data = preprocessing.sequence.pad_sequences(ansBatchToTake, maxlen=maxlen_answers, padding='post')\n    #print(decoder_input_data.shape)\n    # decoder_output_data\n    for i in range(len(ansBatchToTake)):\n        ansBatchToTake[i] = ansBatchToTake[i][1:]\n    decoder_output_data = preprocessing.sequence.pad_sequences(ansBatchToTake, maxlen=maxlen_answers, padding='post')\n    decoder_output_data = utils.to_categorical(decoder_output_data, VOCAB_SIZE)\n\n    indices = np.arange(len(encoder_input_data))\n    np.random.shuffle(indices)\n    encoder_input_data = encoder_input_data[indices]\n    decoder_input_data = decoder_input_data[indices]\n    decoder_output_data = decoder_output_data[indices]    \n    #print(decoder_output_data.shape)\n    model.fit([encoder_input_data, decoder_input_data],\n              decoder_output_data,\n              epochs=1)","execution_count":null,"outputs":[{"output_type":"stream","text":"0 2000\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[{"output_type":"stream","text":"(100000, 299) 299\n(100000, 505) 505\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}